{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "17fe_BlbHLNX"
   },
   "source": [
    "# CS 5683: Project 2\n",
    "Name: Pankajdeer Bikumalla <br>\n",
    "\n",
    "CWID: A20220181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJ5OPxK3HEJo"
   },
   "source": [
    "# Project-2: Locality Sensitive Hashing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qDOCSGAUqCO"
   },
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "from random import randrange\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In0-xXSRHEJ2"
   },
   "source": [
    "## Execute the follwing two cells to generate your data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "puc_dQZoPl5w"
   },
   "outputs": [],
   "source": [
    "# MY_ID should be string\n",
    "MY_ID = '181'\n",
    "# Example MY_ID='819'\n",
    "\n",
    "n_1 = 0\n",
    "n_2 = 0\n",
    "n_3 = 0\n",
    "task_dict = {0:'taska',1:'taskb',2:'taskc',3:'taskd',4:'taske'}\n",
    "# id_dict = {0:n_1,1:n_2,2:n_3}\n",
    "\n",
    "try:\n",
    "    n_1 = int(MY_ID[-1]) % 5\n",
    "    n_2 = int(MY_ID[-2]) % 5\n",
    "    n_3 = int(MY_ID[-3]) % 5\n",
    "    \n",
    "    while n_1 == n_2 or n_1 == n_3:\n",
    "        n_1 = (n_1 + 1) % 5\n",
    "    \n",
    "    while n_1 == n_2 or n_2 == n_3:\n",
    "        n_2 = (n_2 + 1) % 5\n",
    "\n",
    "except Exception as e:\n",
    "    print('Please enter a valid ID...')\n",
    "id_dict = {0:n_1,1:n_2,2:n_3}        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "30jw-rz2Yj_y",
    "outputId": "3940951e-f4af-4dfa-c053-749a51aaa8c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcIWKt9mPoxT"
   },
   "outputs": [],
   "source": [
    "#professor give ($)\n",
    "data_path = '/content/drive/My Drive/3Sem/CS5683/Projects/Project 2/corpus-20090418' \n",
    "\n",
    "# Edit this path if the data directory is not in the current directory\n",
    "try:\n",
    "    os.makedirs('Data_Sample')\n",
    "    os.makedirs('Original_Sample')\n",
    "except Exception as e:\n",
    "    pass\n",
    "\n",
    "for i in range(3):\n",
    "    task = task_dict[id_dict[i]]\n",
    "    \n",
    "    for file in os.listdir(data_path):\n",
    "        if task in file:\n",
    "            if 'orig' in file:\n",
    "                copyfile(data_path + '/' + file, 'Original_Sample/' + file)\n",
    "            else:\n",
    "                copyfile(data_path + '/' + file, 'Data_Sample/' + file)\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-tiMOUMHEKE"
   },
   "source": [
    "Your dataset for this project will be in <b>Data_Sample</b>\n",
    "\n",
    "\n",
    "Your query choices will be in <b>Original_Sample</b> directory\n",
    "\n",
    "\n",
    "You have to use any one original Wikipedia article from <b>Original_Sample</b> for <b>Fact Check</b> steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bTbD1vOAPsVF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6QXXiSHkv0Hr"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlTLTk2EHEKM"
   },
   "source": [
    "### STEP - 1: Shingling (20 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7W2Pt_anPwFz"
   },
   "outputs": [],
   "source": [
    "# converted upper case charaters to lower case\n",
    "prefix = '/content/Data_Sample/'\n",
    "files_list=os.listdir(prefix)\n",
    "# Dict stores, [Keys: document],[values: words in each documents]\n",
    "Dict = {}\n",
    "combined = []\n",
    "for a in files_list:\n",
    "  data = open((prefix+a),'r',encoding ='utf8',errors='ignore').read()\n",
    "  data_words= data.lower().split()\n",
    "  Dict[a]=data_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lp_aHEkNaA1J"
   },
   "outputs": [],
   "source": [
    "# Function to get count of unique shingles (based on k values)\n",
    "# this function returns both count of unique shingles and set(unique shingles) for given values of k\n",
    "\n",
    "def count_shingles(Dict,k,files):\n",
    "  count_shingles = 0\n",
    "  unique_shingles=set()  \n",
    "  for key in files:\n",
    "    shingles = []\n",
    "    for i in range(len(Dict[key])-k+1):\n",
    "      shingles=Dict[key][i:i+k]\n",
    "      shingles = ' '.join(shingles)\n",
    "      if shingles in unique_shingles:\n",
    "        continue\n",
    "      else:\n",
    "        unique_shingles.add(shingles)\n",
    "        count_shingles=count_shingles+1\n",
    "  return count_shingles, unique_shingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HERab1hebBSb",
    "outputId": "aa24b9f2-e120-4b43-805e-8c4a86e42a59"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of unique 3-shingles count is  7929\n",
      "number of unique 4-shingles count is  8721\n",
      "number of unique 5-shingles count is  9120\n"
     ]
    }
   ],
   "source": [
    "#count of 3 shingles\n",
    "count_3shingles,unique_3shingles = count_shingles(Dict,3,os.listdir(prefix))\n",
    "print(\"number of unique 3-shingles count is \", count_3shingles)\n",
    "\n",
    "#count of 4 shingles\n",
    "count_4shingles, unique_4shingles = count_shingles(Dict,4,os.listdir(prefix))\n",
    "print(\"number of unique 4-shingles count is \", count_4shingles)\n",
    "\n",
    "#count of 4 shingles\n",
    "count_5shingles, unique_5shingles = count_shingles(Dict,5,os.listdir(prefix))\n",
    "print(\"number of unique 5-shingles count is \", count_5shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cgDbLjTBHEKT"
   },
   "source": [
    "Report results (number of unique k-shingles) for k={3,4,5} below:\n",
    "1. k=3: 7929\n",
    "2. k=4: 8721\n",
    "3. k=5: 9120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9nl9puei05b"
   },
   "outputs": [],
   "source": [
    "# Type your code to get the 5-shingle index here\n",
    "\n",
    "count_5shingles, unique_5shingles = count_shingles(Dict,5,os.listdir(prefix))\n",
    "#shingles are indexed here from {0 to count_5_uniqueshingles}\n",
    "unique_5shingles_list = list(unique_5shingles)\n",
    "\n",
    "# Storing the unique shingles by document wise\n",
    "dict_5shingles = {}\n",
    "k = 5\n",
    "for file in Dict:\n",
    "  file_5shingles=set()\n",
    "  for i in range(len(Dict[file]) - k + 1): \n",
    "    shingles_5 = Dict[file][i:i + k]\n",
    "    shingles_5 = ' '.join(shingles_5)\n",
    "    if shingles_5 in file_5shingles:\n",
    "      continue\n",
    "    else:\n",
    "      file_5shingles.add(shingles_5)\n",
    "  dict_5shingles[file]=file_5shingles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtBl1eFsHEKe"
   },
   "source": [
    "### STEP - 2: Min-Hashing (40 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0nb8zIpHEKf"
   },
   "outputs": [],
   "source": [
    "# ***************************************************NEW***********************************************************\n",
    "# Generate Hash functions - \n",
    "    # We use (ax + b) mod N formula to permute shingle index\n",
    "    # where a,b are random numbers, N total index size, and x is the index\n",
    "# We need to do L permutations - In other words we need to have L permutations (lists) of new indexes\n",
    "# Following function takes total index size N and L as arguments\n",
    "    # And returns L new lists of size N\n",
    "    \n",
    "def get_hash_functions(N,L):\n",
    "    hash_functions = []\n",
    "    \n",
    "    for itr in range(L):\n",
    "        a=randrange(1,400)\n",
    "        b=randrange(1,400)\n",
    "        \n",
    "        new_hash_function = []\n",
    "        for i in range(N):\n",
    "            new_hash_function.append((a * i + b) % N)\n",
    "        \n",
    "        hash_functions.append(new_hash_function)\n",
    "    return hash_functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VExp5aV6aXAe"
   },
   "source": [
    "### 2a: Generating Hash Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VRrOGW7Ce3zk"
   },
   "outputs": [],
   "source": [
    "# Type your code here to generate all L hash functions\n",
    "# Generate hash functions only for shingle index created for k=5\n",
    "N =count_5shingles\n",
    "\n",
    "L50 = 50\n",
    "H50 = get_hash_functions(N,L50)\n",
    "\n",
    "L100 = 100\n",
    "H100 = get_hash_functions(N,L100)\n",
    "\n",
    "L200 = 200\n",
    "H200 = get_hash_functions(N,L200)\n",
    "\n",
    "L500 = 500\n",
    "H500 = get_hash_functions(N,L500)\n",
    "\n",
    "L1000 = 1000\n",
    "H1000 = get_hash_functions(N,L1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JEaW4ftrb7vf"
   },
   "outputs": [],
   "source": [
    "#for initializing values to infinity\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFrCtpvUkNep"
   },
   "source": [
    "### 2c. Generating Signature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0-sS3ZdwVK1A"
   },
   "outputs": [],
   "source": [
    "# Function to generate: signature matrix\n",
    "#uni_shingles: unique 5 singles\n",
    "#dict_doc_shingles: [keys: file names],[values:shingles in respective files]\n",
    "\n",
    "def sigMatrix(H,uni_shingles,dict_doc_shingles):\n",
    "  list_values = []\n",
    "  for f in dict_doc_shingles:\n",
    "    values_mat = [math.inf for i in range(len(H))] \n",
    "    col = 0\n",
    "    for s in uni_shingles:\n",
    "      if s in list(dict_doc_shingles[f]):\n",
    "        for row in range(len(H)):\n",
    "          if H[row][col] < values_mat[row]:\n",
    "            values_mat[row] = H[row][col]\n",
    "      # dict_columns[f]=col\n",
    "      col=col+1\n",
    "    list_values.append(values_mat)\n",
    "  sig_mat = np.mat(np.array(list_values).T)\n",
    "  return sig_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_4C4YyWVMsE"
   },
   "outputs": [],
   "source": [
    "# Signature matrices\n",
    "# Signature matrix: H50\n",
    "sig_mat_50 = sigMatrix(H50, list(unique_5shingles),dict_5shingles)\n",
    "\n",
    "# Signature matrix: H100\n",
    "sig_mat_100 = sigMatrix(H100, list(unique_5shingles),dict_5shingles)\n",
    "\n",
    "# Signature matrix: H200\n",
    "sig_mat_200 = sigMatrix(H200, list(unique_5shingles),dict_5shingles)\n",
    "\n",
    "# Signature matrix: H500\n",
    "sig_mat_500 = sigMatrix(H500, list(unique_5shingles),dict_5shingles)\n",
    "\n",
    "# Signature matrix: H1000\n",
    "sig_mat_1000 = sigMatrix(H1000, list(unique_5shingles),dict_5shingles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8uidPN6uj-C"
   },
   "source": [
    "### 2d: Fact check \n",
    "Jaccard similarity score greater than t (t=0.85) <br>\n",
    "<br>\n",
    "I have conducted analysis by considering the first file in the \"Original_sample\" folder, for computing jaccard similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zkT2AE7El48v"
   },
   "outputs": [],
   "source": [
    "# Type your code here to do the fact check \n",
    "#      with any one query document in the 'Original_Sample' directory\n",
    "prefix_orig = '/content/Original_Sample/'\n",
    "orig_files=os.listdir(prefix_orig)\n",
    "Dict_orig = {}\n",
    "\n",
    "#picking first file in original data folder for fat check\n",
    "data1 = open((prefix_orig+orig_files[0]),'r',encoding ='utf8',errors='ignore').read()\n",
    "orig_words= data1.lower().split()\n",
    "Dict_orig[orig_files[0]]=orig_words\n",
    "\n",
    "# STEP-1: Generate 5-shingles \n",
    "    # (if any shingles are not present in your shingle index, simply ignore them)\n",
    "first_file =[]\n",
    "first_file.append(os.listdir(prefix_orig)[0])\n",
    "\n",
    "count_5orig_shingles,unique_5orig_shingles= count_shingles(Dict_orig,5,first_file)\n",
    "\n",
    "#ignoring shingles which are not in shingle index\n",
    "lis_orig_5shignles=[]\n",
    "for shingle in list(unique_5orig_shingles):\n",
    "  if shingle in list(unique_5shingles):\n",
    "    lis_orig_5shignles.append(shingle)\n",
    "  else:\n",
    "    continue\n",
    "\n",
    "dict_5orig_shingles = {}\n",
    "dict_5orig_shingles[orig_files[0]]=set(lis_orig_5shignles)\n",
    "\n",
    "# STEP-2: Generate signature vector from L hash functions\n",
    "\n",
    "#Hash 50\n",
    "sig_origmat_50=sigMatrix(H50, list(unique_5shingles),dict_5orig_shingles)\n",
    "\n",
    "#Hash 100\n",
    "sig_origmat_100=sigMatrix(H100, list(unique_5shingles),dict_5orig_shingles)\n",
    "\n",
    "#Hash 200\n",
    "sig_origmat_200=sigMatrix(H200, list(unique_5shingles),dict_5orig_shingles)\n",
    "\n",
    "#Hash 500\n",
    "sig_origmat_500=sigMatrix(H500, list(unique_5shingles),dict_5orig_shingles)\n",
    "\n",
    "#Hash 1000\n",
    "sig_origmat_1000=sigMatrix(H1000, list(unique_5shingles),dict_5orig_shingles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjA4GhoQd16k"
   },
   "outputs": [],
   "source": [
    "# Function: to Compute jaccard similarity\n",
    "#Files: list of files to be compared for computing jacc similarity\n",
    "# matrix: signature matrix\n",
    "#matrix_orig: signature matrix of document from Origanl_sample folder\n",
    "def jaccordsim(files,H,matrix,matrix_orig):\n",
    "  jac_list = []\n",
    "  for i in range(len(files)):\n",
    "    match_count = 0\n",
    "    for j in range(len(H)):\n",
    "      if matrix_orig[j,0]==matrix[j,i]:\n",
    "        match_count = match_count+1\n",
    "    jac_sim = (match_count/len(H))\n",
    "    jac_list.append(jac_sim)\n",
    "  return jac_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8emgzCjlgR7n"
   },
   "outputs": [],
   "source": [
    "# function: to filter documents with jaccard similarity > 0.2\n",
    "# jac_list:is obtained from jaccordsim() function\n",
    "def filter_jaclist(jac_list,files,t):\n",
    "  jac_final = {}\n",
    "  for i,doc in zip(jac_list,files):\n",
    "    if i >=t:\n",
    "      jac_final[doc] = i\n",
    "  return jac_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8HHXHHeggID7"
   },
   "outputs": [],
   "source": [
    "# STEP-3: Calculate Jaccard similarity of signature vector of orginal doc.\n",
    "    # and all other documents     \n",
    "#initializing the jaccard similarity cutoff value\n",
    "t = 0.2\n",
    "\n",
    "\n",
    "# Calculating Jacard Similarity for 50 hash functions\n",
    "jac_list_50= jaccordsim(files_list,H50,sig_mat_50,sig_origmat_50)\n",
    "#filetered documents with jcd>0.2\n",
    "jac_50_final = filter_jaclist(jac_list_50,files_list,t)\n",
    "\n",
    "# Calculating Jacard Similarity for 100 hash functions\n",
    "jac_list_100= jaccordsim(files_list,H100,sig_mat_100,sig_origmat_100)\n",
    "#filetered documents with jcd>0.2\n",
    "jac_100_final = filter_jaclist(jac_list_100,files_list,t)\n",
    "\n",
    "# Calculating Jacard Similarity for 200 hash functions\n",
    "jac_list_200= jaccordsim(files_list,H200,sig_mat_200,sig_origmat_200)\n",
    "#filetered documents with jcd>0.2\n",
    "jac_200_final = filter_jaclist(jac_list_200,files_list,t)\n",
    "\n",
    "# Calculating Jacard Similarity for 500 hash functions\n",
    "jac_list_500= jaccordsim(files_list,H500,sig_mat_500,sig_origmat_500)\n",
    "#filetered documents with jcd>0.2\n",
    "jac_500_final = filter_jaclist(jac_list_500,files_list,t)\n",
    "\n",
    "# Calculating Jacard Similarity for 1000 hash functions\n",
    "jac_list_1000= jaccordsim(files_list,H1000,sig_mat_1000,sig_origmat_1000)\n",
    "#filetered documents with jcd>0.2\n",
    "jac_1000_final = filter_jaclist(jac_list_1000,files_list,t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YskTUfj9HEK1"
   },
   "source": [
    "***************************************************NEW*********************************************************** <br>\n",
    "For each L = {50,100,200,500,1000}, report all documents (file_names) below \n",
    "that have Jaccard similarity > t=0.2\n",
    "\n",
    "Sort the documents in decreasing order of the Jaccard similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "se8tTJvM25CY",
    "outputId": "75ec8478-1eb8-40fe-ce52-719ad4019cb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity for 50 Hash functions : [('g3pA_taskd.txt', 0.82), ('g4pC_taskd.txt', 0.7), ('g2pB_taskd.txt', 0.48), ('g0pC_taskd.txt', 0.36), ('g2pA_taskd.txt', 0.34), ('g0pB_taskd.txt', 0.3), ('g1pA_taskd.txt', 0.26), ('g4pB_taskd.txt', 0.26), ('g2pC_taskb.txt', 0.22), ('g1pA_taskb.txt', 0.2)]\n",
      "\n",
      "\n",
      "Jaccard Similarity for 100 Hash functions : [('g3pA_taskd.txt', 0.89), ('g4pC_taskd.txt', 0.66), ('g2pB_taskd.txt', 0.43), ('g1pA_taskd.txt', 0.32), ('g0pC_taskd.txt', 0.32), ('g2pA_taskd.txt', 0.31), ('g2pC_taskd.txt', 0.27), ('g4pB_taskd.txt', 0.26), ('g0pB_taskd.txt', 0.24)]\n",
      "\n",
      "\n",
      "Jaccard Similarity for 200 Hash functions : [('g3pA_taskd.txt', 0.825), ('g4pC_taskd.txt', 0.775), ('g2pB_taskd.txt', 0.47), ('g0pC_taskd.txt', 0.405), ('g2pA_taskd.txt', 0.33), ('g4pB_taskd.txt', 0.295), ('g0pB_taskd.txt', 0.25), ('g1pA_taskd.txt', 0.245), ('g2pC_taskd.txt', 0.215)]\n",
      "\n",
      "\n",
      "Jaccard Similarity for 500 Hash functions : [('g3pA_taskd.txt', 0.806), ('g4pC_taskd.txt', 0.702), ('g2pB_taskd.txt', 0.478), ('g0pC_taskd.txt', 0.378), ('g2pA_taskd.txt', 0.294), ('g1pA_taskd.txt', 0.274), ('g4pB_taskd.txt', 0.26), ('g0pB_taskd.txt', 0.236), ('g2pC_taskd.txt', 0.206)]\n",
      "\n",
      "\n",
      "Jaccard Similarity for 1000 Hash functions : [('g3pA_taskd.txt', 0.814), ('g4pC_taskd.txt', 0.702), ('g2pB_taskd.txt', 0.464), ('g0pC_taskd.txt', 0.369), ('g2pA_taskd.txt', 0.287), ('g1pA_taskd.txt', 0.268), ('g4pB_taskd.txt', 0.245), ('g0pB_taskd.txt', 0.203)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Jaccard Similarity for 50 Hash functions :\", sorted(jac_50_final.items(),key=lambda x: x[1], reverse=True))\n",
    "print('\\n')\n",
    "print(\"Jaccard Similarity for 100 Hash functions :\", sorted(jac_100_final.items(),key=lambda x: x[1], reverse=True))\n",
    "print('\\n')\n",
    "print(\"Jaccard Similarity for 200 Hash functions :\", sorted(jac_200_final.items(),key=lambda x: x[1], reverse=True))\n",
    "print('\\n')\n",
    "print(\"Jaccard Similarity for 500 Hash functions :\", sorted(jac_500_final.items(),key=lambda x: x[1], reverse=True))\n",
    "print('\\n')\n",
    "\n",
    "print(\"Jaccard Similarity for 1000 Hash functions :\", sorted(jac_1000_final.items(),key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlUyFeU4HEK5"
   },
   "source": [
    "### STEP - 3: LSH (30 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RsoRVHZejUEv"
   },
   "outputs": [],
   "source": [
    "# Type your code here to hash signature matrix into B buckets\n",
    "# Use the technique to split the signature matrix into b bands of r rows\n",
    "# Convert only the signature matrix generated with L=1000\n",
    "\n",
    "b = 50\n",
    "r = 20\n",
    "B = 199"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yvPLXl8K01Vd"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fp1FjPWqxX32"
   },
   "source": [
    "### 3a. Hash Signature Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y4JxWWG4eGfN"
   },
   "outputs": [],
   "source": [
    "# Type your code here to do generate candidate documents\n",
    "# Follow all steps from STEP - 2 fact check (except the Jaccard similarity part)\n",
    "\n",
    "# STEP - 1: Split your original document signature vector into b bands of r rows\n",
    "sig_mat_arr = np.array(sig_mat_1000).reshape(b,r,-1)\n",
    "\n",
    "# generating a values - based on the condition mentioned\n",
    "a = random.sample(list(np.arange(start = 1,stop=r+1,step =1)),r)\n",
    "\n",
    "\n",
    "# STEP - 2: Hash using the same hash functions created for \n",
    "    # signature matrix hashing (in the previous cell)\n",
    "bucket = {}\n",
    "for i in range(sig_mat_arr.shape[0]):\n",
    "  Dict_hash=collections.defaultdict(list)\n",
    "  for k in range(sig_mat_arr.shape[2]):\n",
    "    h =np.sum(a*(sig_mat_arr[i,:,k])) % B\n",
    "    Dict_hash[h].append(files_list[k])\n",
    "  bucket.update(Dict_hash)\n",
    "\n",
    "sig_origmat_1000_arr =np.array(sig_origmat_1000).reshape(50,20,-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POPTQusgxmDR"
   },
   "source": [
    "### 3b. Fact Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dO5Ow0B05xKv"
   },
   "outputs": [],
   "source": [
    "# Type your code here to do the fact check\n",
    "t=0.2\n",
    "# Getting all the candidate items \n",
    "cand_items=set()\n",
    "for i in range(sig_origmat_1000_arr.shape[0]):\n",
    "  h =np.sum(a*(sig_origmat_1000_arr[i,:,0])) % B\n",
    "  for key,value in bucket.items():\n",
    "    if key ==h:\n",
    "      for j in value:\n",
    "        cand_items.add(j)\n",
    "\n",
    "k = 5\n",
    "cand_list = list(cand_items)\n",
    "#storing shingles in cand_list document wise\n",
    "dict_5shingles_3b = {}\n",
    "for file in cand_list:\n",
    "  dict_5shingles_3b[file] = dict_5shingles[file]\n",
    "\n",
    "\n",
    "# Signature matrix: H1000_3b\n",
    "sig_mat_1000_3b = sigMatrix(H1000, list(unique_5shingles),dict_5shingles_3b)\n",
    "\n",
    "\n",
    "# Calculating Jacard Similarity for 1000 hash functions\n",
    "jac_list_1000_3b= jaccordsim(cand_list,H1000,sig_mat_1000_3b,sig_origmat_1000)\n",
    "#filetered documents with jcd>0.2\n",
    "jac_1000_final_3b = filter_jaclist(jac_list_1000_3b,cand_list,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "_98fYdylF9Df"
   },
   "outputs": [],
   "source": [
    "# Obtaining False positives and false negatives based on the reference from project-2 document\n",
    "## False Positives\n",
    "FP=[]\n",
    "for file in cand_list:\n",
    "  if file in list(jac_1000_final_3b.keys()):\n",
    "    continue\n",
    "  else: \n",
    "    FP.append(file)\n",
    "\n",
    "## False Negatives\n",
    "FN=[]\n",
    "for file in list(jac_1000_final.keys()):\n",
    "  if file in list(jac_1000_final_3b.keys()):\n",
    "    continue\n",
    "  else: \n",
    "    FN.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srZyy2m4HELM"
   },
   "outputs": [],
   "source": [
    "# Obtaining False positives and false negatives based on the reference from project-2 document\n",
    "## False Positives\n",
    "FP=[]\n",
    "for file in cand_list:\n",
    "  if file in list(jac_1000_final_3b.keys()):\n",
    "    continue\n",
    "  else: \n",
    "    FP.append(file)\n",
    "\n",
    "## False Negatives\n",
    "FN=[]\n",
    "for file in list(jac_1000_final.keys()):\n",
    "  if file in list(jac_1000_final_3b.keys()):\n",
    "    continue\n",
    "  else: \n",
    "    FN.append(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8oTWL7QHELO"
   },
   "source": [
    "Report all documents (file_names) below that have Jaccard similarity > t=0.2\n",
    "Sort the documents in decreasing order of the Jaccard similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JXw1MJQfHELO",
    "outputId": "d783c602-8803-4da7-fc89-8be643bcdb4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jaccard Similarity for 1000 Hash functions with t>=0.2 are :  \n",
      "\n",
      "[('g3pA_taskd.txt', 0.814), ('g4pC_taskd.txt', 0.702), ('g0pC_taskd.txt', 0.369), ('g2pA_taskd.txt', 0.287), ('g1pA_taskd.txt', 0.268), ('g4pB_taskd.txt', 0.245), ('g0pB_taskd.txt', 0.203)]\n"
     ]
    }
   ],
   "source": [
    "print(\"Jaccard Similarity for 1000 Hash functions with t>=0.2 are : \", \"\\n\")\n",
    "print( sorted(jac_1000_final_3b.items(),key=lambda x: x[1], reverse=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l18YkwUGHELQ"
   },
   "source": [
    "Report the list of false positives and false negatives below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z3q4d8mSHELR",
    "outputId": "a5a239d8-db08-4be7-e803-4975f8714143"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of Flase positives are : 28\n",
      " Documents which are Flase positives are : ['g0pD_taskd.txt', 'g4pE_taskb.txt', 'g2pB_taskc.txt', 'g0pB_taskb.txt', 'g2pA_taskc.txt', 'g1pB_taskb.txt', 'g3pB_taskc.txt', 'g2pC_taskc.txt', 'g2pA_taskb.txt', 'g4pE_taskd.txt', 'g0pD_taskc.txt', 'g4pD_taskb.txt', 'g4pE_taskc.txt', 'g4pB_taskb.txt', 'g3pA_taskc.txt', 'g0pE_taskc.txt', 'g0pC_taskb.txt', 'g1pD_taskb.txt', 'g4pC_taskb.txt', 'g2pC_taskb.txt', 'g1pB_taskc.txt', 'g2pE_taskc.txt', 'g3pB_taskb.txt', 'g2pE_taskd.txt', 'g0pE_taskb.txt', 'g3pC_taskc.txt', 'g4pC_taskc.txt', 'g0pB_taskc.txt']\n",
      "\n",
      "\n",
      " Number of Flase Negatives are : 1\n",
      " Documents which are Flase Negatives are : ['g2pB_taskd.txt']\n"
     ]
    }
   ],
   "source": [
    "print(\" Number of Flase positives are :\", len(FP))\n",
    "print(\" Documents which are Flase positives are :\", FP)\n",
    "print('\\n')\n",
    "\n",
    "print(\" Number of Flase Negatives are :\", len(FN))\n",
    "print(\" Documents which are Flase Negatives are :\", FN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_m95qEXHELT"
   },
   "source": [
    "### BONUS : 10 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nOwtsmzqy02F"
   },
   "source": [
    "To perform the experiment in the bonus task section, I have created a function which takes the signature_matrix computed in the previous steps by inheritence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vak580RrHELU"
   },
   "outputs": [],
   "source": [
    "# Experiment with different values of b,r,B, and t\n",
    "    # to reduce the number of false positives and false negatives\n",
    "    # Report all results in a table in a separate word document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InS8Z81lzIl3"
   },
   "source": [
    "#### For different combinations of b(bands),r(rows),B(bucket):\n",
    "the following function return the count of false positives and false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9ioKQ3Gda5kZ"
   },
   "outputs": [],
   "source": [
    "# Function which can give us output of false posities, negatives\n",
    "# we can vary b,r,B values only \n",
    "# a is generated randomly everytime\n",
    "def experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000):\n",
    "  # STEP - 1: Split your original document signature vector into b bands of r rows\n",
    "  sig_mat_arr = np.array(sig_mat_1000).reshape(b,r,-1)\n",
    "\n",
    "  # STEP - 2: Hash using the same hash functions created for \n",
    "      # signature matrix hashing (in the previous cell)\n",
    "\n",
    "  a = random.sample(list(np.arange(start = 1,stop=r+1,step =1)),r)\n",
    "  bucket = {}\n",
    "  for i in range(sig_mat_arr.shape[0]):\n",
    "    Dict_hash=collections.defaultdict(list)\n",
    "    for k in range(sig_mat_arr.shape[2]):\n",
    "      h =np.sum(a*(sig_mat_arr[i,:,k])) % B\n",
    "      Dict_hash[h].append(files_list[k])\n",
    "    bucket.update(Dict_hash)\n",
    "\n",
    "  sig_origmat_1000_arr =np.array(sig_origmat_1000).reshape(b,r,-1)\n",
    "\n",
    "  # Getting all the candidate items \n",
    "  cand_items=set()\n",
    "  for i in range(sig_origmat_1000_arr.shape[0]):\n",
    "    h =np.sum(a*(sig_origmat_1000_arr[i,:,0])) % B\n",
    "    for key,value in bucket.items():\n",
    "      if key ==h:\n",
    "        for j in value:\n",
    "          cand_items.add(j)\n",
    "  # Type your code here to do the fact check\n",
    "\n",
    "  k = 5\n",
    "  #storing shingles in cand_list document wise\n",
    "  cand_list = list(cand_items)\n",
    "  print(\"Count of candidate documents are: \",len(cand_list))\n",
    "  dict_5shingles_3b = {}\n",
    "  for file in cand_list:\n",
    "    dict_5shingles_3b[file] = dict_5shingles[file]\n",
    "\n",
    "\n",
    "  # Signature matrix: H1000_3b\n",
    "  sig_mat_1000_3b = sigMatrix(H1000, list(unique_5shingles),dict_5shingles_3b)\n",
    "\n",
    "  jac_1000_final = filter_jaclist(jac_list_1000,files_list,t)\n",
    "  # Calculating Jacard Similarity for 1000 hash functions\n",
    "  jac_list_1000_3b= jaccordsim(cand_list,H1000,sig_mat_1000_3b,sig_origmat_1000)\n",
    "  print(\"count of documents with t>0.2 are: \", len(list(jac_1000_final.keys())))\n",
    "  #filetered documents with jcd>0.2\n",
    "  jac_1000_final_3b = filter_jaclist(jac_list_1000_3b,cand_list,t)\n",
    "  print(\"count of cadidate documents with t>0.2 are: \", len(list(jac_1000_final_3b.keys())))\n",
    "\n",
    "  ## False Positives\n",
    "  FP=[]\n",
    "  for file in cand_list:\n",
    "    if file in list(jac_1000_final.keys()):\n",
    "      continue\n",
    "    else: \n",
    "      FP.append(file)\n",
    "  print(\"false positive's count: \", len(FP))\n",
    "\n",
    "  ## False Negatives\n",
    "  FN=[]\n",
    "  for file in list(jac_1000_final.keys()):\n",
    "    if file in list(jac_1000_final_3b.keys()):\n",
    "      continue\n",
    "    else: \n",
    "      FN.append(file)\n",
    "  print(\"false negtive's count: \", len(FN))\n",
    "  return FP, FN, len(FP),len(FN), list(jac_1000_final.keys()),list(jac_1000_final_3b.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EVGOvUWUPNxz",
    "outputId": "a711762a-ca47-46fd-b2f4-d1e542b5dec1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  21\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  2\n",
      "false positive's count:  19\n",
      "false negtive's count:  6\n"
     ]
    }
   ],
   "source": [
    "# Combination 1\n",
    "b = 25\n",
    "r = 40\n",
    "B = 200\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Un8xQn9cjsAL",
    "outputId": "43437449-6f9f-461b-a31c-f17d31bb448a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  21\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  4\n",
      "false positive's count:  17\n",
      "false negtive's count:  4\n"
     ]
    }
   ],
   "source": [
    "# Combination 2\n",
    "b = 25\n",
    "r = 40\n",
    "B = 300\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iwv71aKMj0Jf",
    "outputId": "dd708f55-3b5d-4ca8-f364-aaf8eedfee23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  15\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  1\n",
      "false positive's count:  14\n",
      "false negtive's count:  7\n"
     ]
    }
   ],
   "source": [
    "# Combination 3\n",
    "b = 20\n",
    "r = 50\n",
    "B = 400\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QX8y-3MsOwaM",
    "outputId": "d9557b33-d7df-46c4-9fcc-ddb105cf4157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  20\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  2\n",
      "false positive's count:  18\n",
      "false negtive's count:  6\n"
     ]
    }
   ],
   "source": [
    "# Combination 4\n",
    "b = 20\n",
    "r = 50\n",
    "B = 200\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpDNJPytkn0C",
    "outputId": "d83f13c5-9a3c-4a8c-b233-01d4555e7499"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  17\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  2\n",
      "false positive's count:  15\n",
      "false negtive's count:  6\n"
     ]
    }
   ],
   "source": [
    "# Combination 5\n",
    "b = 20\n",
    "r = 50\n",
    "B = 300\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4uF23xuE1Ph3",
    "outputId": "89503e61-3431-4570-f4a2-fa85950e20b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  16\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  2\n",
      "false positive's count:  14\n",
      "false negtive's count:  6\n"
     ]
    }
   ],
   "source": [
    "# Combination 6\n",
    "b = 20\n",
    "r = 50\n",
    "B = 400\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DdxIlBLekwso",
    "outputId": "904c3d96-4e1e-4ffa-e2bf-ddb76c1cfeec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  10\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  2\n",
      "false positive's count:  8\n",
      "false negtive's count:  6\n"
     ]
    }
   ],
   "source": [
    "# Combination 7\n",
    "b = 10\n",
    "r = 100\n",
    "B = 200\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bITiuCtCk0vb",
    "outputId": "75a2ae50-c508-4734-e9a9-63e194bb1208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  9\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  1\n",
      "false positive's count:  8\n",
      "false negtive's count:  7\n"
     ]
    }
   ],
   "source": [
    "# Combination 8\n",
    "b = 10\n",
    "r = 100\n",
    "B = 300\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nW-eKECxk5AB",
    "outputId": "515af786-195a-4e6b-c90b-ccbaa2c91a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  8\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  1\n",
      "false positive's count:  7\n",
      "false negtive's count:  7\n"
     ]
    }
   ],
   "source": [
    "# Combination 9\n",
    "b = 10\n",
    "r = 100\n",
    "B = 400\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mnKFzYNa12HS",
    "outputId": "4b2e5cb5-e000-470f-e02d-b2d0f80da474"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of candidate documents are:  5\n",
      "count of documents with t>0.2 are:  8\n",
      "count of cadidate documents with t>0.2 are:  2\n",
      "false positive's count:  3\n",
      "false negtive's count:  6\n"
     ]
    }
   ],
   "source": [
    "# Combination 9\n",
    "b = 10\n",
    "r = 100\n",
    "B = 500\n",
    "t=0.2\n",
    "x = experiment(b,r,B,t,sig_mat_1000,sig_origmat_1000,dict_5shingles,unique_5shingles,H1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qsva6DVd3SVJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZP0z-xeU2dWr"
   },
   "source": [
    "### References:\n",
    "1. https://pynative.com/python-random-sample/\n",
    "2. https://note.nkmk.me/en/python-random-choice-sample-choices/\n",
    "3. https://github.com/chrisjmccormick/MinHash/blob/master/runMinHashExample.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qo9WnpsF2fKj"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Project_2_Pankajdeer_CS5683.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
